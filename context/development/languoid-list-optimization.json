{
  "languoid_list_optimization": {
    "version": "1.2",
    "last_updated": "2025-10-28",
    "conversation_reference": "Languoid list page performance and hierarchical display optimization + URL routing consistency fix + Smart caching implementation",
    
    "overview": {
      "scope": "Comprehensive refactoring of the Languoid list page (LanguoidsList.tsx) to implement frontend-only filtering, smart pagination, and hierarchical tree display optimizations",
      "motivation": "Address performance issues and improve user experience for hierarchical languoid browsing",
      "warning": "⚠️ CRITICAL: These patterns are LANGUOID-SPECIFIC and should NOT be applied to other model list pages (Collections, Items, Documents, Collaborators) without explicit approval from the project manager"
    },
    
    "changes_implemented": {
      "1_frontend_only_filtering": {
        "change_id": "languoid_list_001",
        "change_type": "performance_optimization",
        "specific_to": "Languoid list page only",
        "description": "Refactored to load all languoids once and perform filtering entirely on the frontend",
        "technical_details": {
          "state_changes": [
            "Introduced `allLanguoids` state to store complete unfiltered dataset",
            "Created `filteredLanguoids` useMemo that applies search/filter criteria in-memory",
            "Updated `hierarchicalLanguoids` to depend on `filteredLanguoids`"
          ],
          "api_changes": [
            "Modified `loadLanguoids` to fetch with `page_size: '10000'`",
            "Removed filter dependencies from `loadLanguoids` (only fetches once on mount)",
            "No API calls triggered by search/filter changes"
          ],
          "filtering_criteria": [
            "Level preset filter (all, families, languages_dialects, etc.)",
            "Advanced level filter",
            "Search term (name, ISO, glottocode, region, tribes)",
            "Family filter",
            "Region filter"
          ]
        },
        "benefits": [
          "Instant filtering with no API calls",
          "Significantly improved performance",
          "Reduced server load"
        ],
        "applicability_constraints": {
          "why_languoid_specific": "Languoids have a bounded, relatively small dataset (~10,000 max) that fits comfortably in browser memory",
          "dataset_size_assumption": "Maximum ~10,000 languoid records",
          "do_not_apply_to": [
            "Items (could be 100,000+ records)",
            "Collections (unknown scale)",
            "Documents (unknown scale)",
            "Collaborators (unknown scale)"
          ],
          "decision_rule": "Before applying this pattern to other models, consult project manager and verify dataset size constraints"
        },
        "file_modified": "frontend/src/components/languoids/LanguoidsList.tsx",
        "status": "completed"
      },
      
      "2_hierarchical_tree_fix": {
        "change_id": "languoid_list_002",
        "change_type": "bug_fix",
        "description": "Fixed hierarchical tree building logic that caused indentation to stop working after first few families",
        "bug_symptoms": [
          "Indentation worked for first few families",
          "Subsequent families showed incorrect indentation levels",
          "Descendants appeared at wrong hierarchy levels"
        ],
        "root_cause": "The `children` filter was using both `l.family_languoid === languoid.id` AND `l.parent_languoid === languoid.id`, causing incorrect tree structure where descendants were being matched multiple times",
        "fix_implemented": {
          "before": "children = filteredLanguoids.filter(l => !processed.has(l.id) && (l.parent_languoid === languoid.id || l.family_languoid === languoid.id))",
          "after": "children = filteredLanguoids.filter(l => !processed.has(l.id) && l.parent_languoid === languoid.id)",
          "explanation": "Only use parent_languoid FK to identify direct children, ensuring correct parent→child traversal"
        },
        "result": "Correct hierarchical indentation throughout the entire list",
        "file_modified": "frontend/src/components/languoids/LanguoidsList.tsx",
        "status": "completed"
      },
      
      "3_smart_pagination": {
        "change_id": "languoid_list_003",
        "change_type": "feature_enhancement",
        "specific_to": "Languoid list page only",
        "description": "Implemented dynamic page sizing that respects hierarchical boundaries to prevent 'stranding' descendants on subsequent pages",
        "technical_implementation": {
          "approach": "Dynamic page breaks calculated at family boundaries",
          "key_changes": [
            "Replaced fixed `pageSize` with `minPageSize` (50)",
            "Calculates `pageBreaks` dynamically based on top-level family boundaries (indentLevel === 0)",
            "Each page ends at a family boundary to keep complete trees together",
            "`totalPages` derived from dynamic `pageBreaks`"
          ],
          "algorithm": [
            "Start with currentIndex = 0",
            "Calculate breakPoint = currentIndex + minPageSize",
            "Search forward from breakPoint to find next item with indentLevel === 0",
            "If found, set page break at that index",
            "If not found, set page break at end of list",
            "Repeat until all items processed"
          ]
        },
        "benefits": [
          "Complete family trees stay together across pages",
          "No descendants stranded on subsequent pages",
          "Pages vary in size but maintain semantic coherence"
        ],
        "applicability_constraints": {
          "why_languoid_specific": "Smart pagination is designed specifically for Languoid's hierarchical display where maintaining parent-child relationships across pages is critical for user comprehension",
          "requirements_for_applicability": [
            "Data must have clear hierarchical structure with identifiable top-level nodes",
            "Preserving complete subtrees is essential for user understanding",
            "Frontend has access to full dataset for boundary calculation"
          ],
          "do_not_apply_to": [
            "Non-hierarchical lists (Items, Collections, Collaborators, Documents)",
            "Lists with backend pagination only",
            "Lists where subtree coherence is not important"
          ],
          "decision_rule": "Do NOT apply this pattern to other model list pages without consulting project manager, as it adds complexity that may not be warranted for non-hierarchical data"
        },
        "file_modified": "frontend/src/components/languoids/LanguoidsList.tsx",
        "status": "completed"
      },
      
      "4_scroll_behavior_fix": {
        "change_id": "languoid_list_004",
        "change_type": "bug_fix",
        "description": "Fixed inconsistent scroll-to-top behavior when changing pages",
        "bug_symptoms": [
          "Sometimes page scrolled to top of list as expected",
          "Sometimes page did not scroll",
          "No clear pattern for when scrolling occurred"
        ],
        "root_cause": "window.scrollTo was called immediately, potentially before new page content rendered",
        "fix_implemented": {
          "before": "window.scrollTo({ top: 0, behavior: 'smooth' }) in handlePageChange",
          "after": [
            "Added `listTopRef` useRef attached to results counter Box",
            "Added useEffect that watches `displayPage` changes",
            "Calls `listTopRef.current.scrollIntoView({ behavior: 'smooth', block: 'start' })` after render"
          ],
          "explanation": "useEffect ensures scroll happens AFTER new page content has rendered, and scrollIntoView is more reliable than window.scrollTo"
        },
        "result": "Consistent smooth scroll to top after new page content renders",
        "applicability": {
          "reusability": "This scroll pattern is general-purpose and could be applied to other list pages if similar issues arise",
          "pattern_established": "Use useRef + useEffect + scrollIntoView for reliable post-render scrolling"
        },
        "file_modified": "frontend/src/components/languoids/LanguoidsList.tsx",
        "status": "completed"
      },
      
      "5_languages_dialects_tree": {
        "change_id": "languoid_list_005",
        "change_type": "feature_enhancement",
        "specific_to": "Languoid list page only",
        "description": "Special hierarchical handling for 'Languages & Dialects' filter preset",
        "technical_implementation": {
          "detection": "Check if selectedLevelFilter === 'languages_dialects'",
          "tree_building_logic": [
            "Languages shown as root nodes (indentLevel 0), alphabetically sorted",
            "Dialects nested under their parent languages (indented)",
            "Orphaned dialects (no parent language in filtered set) shown at root level",
            "Normal hierarchy (families→subfamilies→languages→dialects) used for other presets"
          ],
          "code_location": "hierarchicalLanguoids useMemo in LanguoidsList.tsx"
        },
        "result": "Clean language→dialect tree view without family/subfamily clutter",
        "applicability_constraints": {
          "why_languoid_specific": "This conditional tree-building logic is entirely specific to Languoid's hierarchical structure and the domain-specific relationship between families, subfamilies, languages, and dialects",
          "domain_specific": "Linguistics domain: family→subfamily→language→dialect hierarchy",
          "not_applicable_to": "All other models (completely domain-specific logic)"
        },
        "file_modified": "frontend/src/components/languoids/LanguoidsList.tsx",
        "status": "completed"
      },
      
      "6_results_counter_update": {
        "change_id": "languoid_list_006",
        "change_type": "ui_enhancement",
        "description": "Updated results counter to show dynamic page range and total counts",
        "implementation": [
          "Shows dynamic page range: pageBreaks[displayPage - 1] + 1 to pageBreaks[displayPage]",
          "Shows hierarchicalLanguoids.length (filtered count)",
          "Shows items per page when exceeding minPageSize",
          "Shows allLanguoids.length as total count when filtering is active"
        ],
        "example_display": "Showing 1-73 of 250 languoids (73 on this page) • 1000 total",
        "applicability": {
          "tied_to": "Frontend-only filtering approach (change #1)",
          "reusability": "Only use where single-load + client-side filtering is appropriate"
        },
        "file_modified": "frontend/src/components/languoids/LanguoidsList.tsx",
        "status": "completed"
      }
    },
    
    "technical_patterns_established": {
      "frontend_pagination_with_dynamic_sizing": {
        "pattern_name": "Smart pagination for hierarchical data",
        "applicability": "LANGUOID-SPECIFIC - requires bounded dataset + hierarchical structure",
        "implementation_requirements": [
          "Frontend has access to full dataset",
          "Data has clear hierarchical structure with identifiable boundaries",
          "Maintaining complete subtrees across pages is critical for UX"
        ],
        "do_not_apply_without_approval": true
      },
      "single_load_client_side_filtering": {
        "pattern_name": "Load once, filter in-memory",
        "applicability": "LANGUOID-SPECIFIC - only for small, bounded datasets",
        "dataset_size_constraint": "Maximum ~10,000 records",
        "benefits": [
          "Instant filtering",
          "No API calls for filter changes",
          "Reduced server load"
        ],
        "risks_for_large_datasets": [
          "Browser memory exhaustion",
          "Initial page load delay",
          "Poor performance with 100,000+ records"
        ],
        "do_not_apply_without_approval": true
      },
      "conditional_tree_building": {
        "pattern_name": "Filter-specific tree construction logic",
        "applicability": "LANGUOID-SPECIFIC - domain-specific hierarchy",
        "implementation": "Different root node selection based on active filter preset",
        "use_case": "Show different hierarchical views (full tree vs. language→dialect subset)",
        "do_not_apply_without_approval": true
      },
      "reliable_scroll_behavior": {
        "pattern_name": "useRef + useEffect + scrollIntoView for post-render scrolling",
        "applicability": "GENERAL-PURPOSE - reusable across all list pages",
        "implementation": [
          "Create ref attached to scroll target element",
          "useEffect watches pagination state",
          "scrollIntoView called after render completes"
        ],
        "can_be_reused": true
      }
    },
    
    "architecture_decision_record": {
      "why_these_patterns_are_languoid_specific": {
        "dataset_size": "Languoids have a bounded, relatively small dataset (~10,000 max) that fits comfortably in browser memory",
        "hierarchical_relationships": "The parent-child tree structure requires special handling to maintain visual coherence across pages",
        "domain_complexity": "The family→subfamily→language→dialect hierarchy is unique to the linguistics domain and drives special display requirements",
        "user_expectations": "Linguists expect to see complete language families without pagination breaks, as this mirrors how linguistic classification systems are organized"
      },
      "before_applying_to_other_models": {
        "models_to_consider": [
          "Collections",
          "Items",
          "Documents",
          "Collaborators"
        ],
        "evaluation_criteria": [
          "Dataset size (Items could be 100,000+, which would break frontend-only filtering)",
          "Whether hierarchical relationships exist and matter to users",
          "Backend pagination/filtering performance vs. frontend memory constraints",
          "User expectations for data organization and display"
        ],
        "mandatory_step": "Always consult the project manager before deviating from standard backend-paginated list patterns"
      },
      "standard_pattern_for_other_models": {
        "default_approach": "Backend pagination with API-driven filtering",
        "page_size": "Fixed page size (typically 20-50 items)",
        "filtering": "Backend applies filters, returns paginated results",
        "performance": "Scales to datasets of any size",
        "when_to_use": "Default for all models unless explicitly approved otherwise"
      }
    },
    
    "files_modified": [
      "frontend/src/components/languoids/LanguoidsList.tsx",
      "frontend/src/components/languoids/LanguoidDetail.tsx",
      "frontend/src/components/languoids/LanguoidCreate.tsx",
      "frontend/src/App.tsx",
      "frontend/src/components/Navigation.tsx",
      "app/archive/urls.py",
      "app/metadata/views.py",
      "app/templates/languoid_index.html"
    ],
    
    "url_routing_consistency_fix": {
      "change_id": "languoid_url_001",
      "change_type": "consistency_improvement",
      "date_implemented": "2025-10-28",
      "description": "Changed all React frontend routes from /languages/* to /languoids/* to match the model name",
      "rationale": "URLs should match model names for developer clarity and API consistency. User-facing labels (like 'Languages' in navigation) remain user-friendly while URLs align with backend model naming",
      "changes_made": {
        "frontend_routes": "App.tsx route changed from /languages/* to /languoids/*",
        "navigation_links": "Navigation.tsx path changed from /languages to /languoids (label still shows 'Languages')",
        "component_navigation": "All navigate() calls in LanguoidsList, LanguoidDetail, LanguoidCreate updated to use /languoids/*",
        "django_routes": "Django urls.py React routes changed from /languages/ to /languoids/ to match",
        "import_route": "Changed /languages/import/ to /languoids/import/",
        "redirect_fix": "Updated metadata/views.py redirect from /languages/ to /languoids/",
        "legacy_template": "Updated languoid_index.html API call from /api/languages/ to /api/v1/languoids/"
      },
      "context_updates": [
        "context/development/stage_0_react_migration.json - Updated terminology_updates section",
        "context/boundaries/api_classification.json - Updated import paths",
        "context/boundaries/preservation_rules.json - Updated import interface paths"
      ],
      "consistency_achieved": "All URLs now consistently use /languoids to match the Languoid model name throughout backend and frontend",
      "user_facing_impact": "Navigation label still displays 'Languages' for user-friendliness while URLs are developer-friendly",
      "status": "completed"
    },
    
    "smart_caching_implementation": {
      "change_id": "languoid_cache_001",
      "change_type": "performance_optimization",
      "date_implemented": "2025-10-28",
      "description": "Implemented production-ready smart caching system for Languoid list page to eliminate 3-second reload on navigation",
      "motivation": "List page was reloading full dataset (10,000 records) every time user navigated back from detail view, causing 3-second delay",
      
      "implementation_approach": "Option 1 (Context Cache) + Option 2 (Smart Invalidation)",
      
      "components": {
        "1_backend_endpoint": {
          "file": "app/internal_api/views.py",
          "endpoint": "GET /internal/v1/languoids/last-modified/",
          "description": "Lightweight endpoint returning most recent update timestamp across all languoids",
          "implementation": "Uses Django aggregate: SELECT MAX(updated), COUNT(id) FROM languoid",
          "response": {
            "last_modified": "ISO 8601 timestamp",
            "count": "Total number of languoids"
          },
          "performance": "< 50ms query, single aggregation"
        },
        
        "2_cache_context": {
          "file": "frontend/src/contexts/LanguoidCacheContext.tsx",
          "description": "React Context provider managing languoid list cache with smart invalidation",
          "features": [
            "In-memory cache with sessionStorage persistence",
            "10-minute TTL (time-to-live)",
            "Automatic timestamp-based validation on access",
            "Manual refresh capability",
            "Survives page refreshes within same browser tab"
          ],
          "cache_structure": {
            "languoids": "Array<Languoid> - full list",
            "lastModified": "string - backend timestamp",
            "cachedAt": "number - client timestamp",
            "count": "number - total languoids"
          },
          "cache_validation_logic": [
            "Check 1: TTL expiration (10 minutes client-side)",
            "Check 2: Backend timestamp changed (smart invalidation)",
            "If both pass: Use cached data (instant)",
            "If either fails: Fetch fresh data"
          ]
        },
        
        "3_frontend_integration": {
          "file": "frontend/src/components/languoids/LanguoidsList.tsx",
          "changes": [
            "Replaced direct API call with useLanguoidCache() hook",
            "Added handleRefresh() for manual cache invalidation",
            "Added refresh IconButton with loading state to header",
            "Cache context wrapped in App.tsx provider tree"
          ],
          "user_experience": [
            "First load: 3 seconds (normal API fetch)",
            "Return navigation: < 100ms (cached data)",
            "After edits: Auto-detects changes via timestamp",
            "Manual refresh: IconButton in header"
          ]
        },
        
        "4_api_method": {
          "file": "frontend/src/services/api.ts",
          "method": "languoidsAPI.getLastModified()",
          "returns": "Promise<{ last_modified: string; count: number }>"
        }
      },
      
      "cache_invalidation_scenarios": {
        "automatic": [
          "10 minutes elapsed (TTL)",
          "Backend last_modified timestamp changed (detected on access)",
          "Page refresh after cache expiry"
        ],
        "manual": [
          "User clicks refresh button",
          "Direct call to refreshCache() from context"
        ],
        "what_triggers_backend_timestamp_update": [
          "Any Languoid model save (auto_now=True on updated field)",
          "Create, update, delete operations",
          "Bulk imports",
          "Signal-triggered updates"
        ]
      },
      
      "performance_metrics": {
        "before": {
          "first_load": "~3 seconds",
          "return_navigation": "~3 seconds (full reload)",
          "filter_changes": "Instant (frontend filtering already implemented)"
        },
        "after": {
          "first_load": "~3 seconds (unchanged - necessary initial fetch)",
          "return_navigation": "< 100ms (cached data)",
          "filter_changes": "Instant (unchanged)",
          "timestamp_check": "< 50ms (lightweight query)",
          "cache_hit_rate": "Expected 90%+ for typical workflows"
        }
      },
      
      "sessionStorage_persistence": {
        "benefit": "Cache survives page refreshes within same browser tab",
        "key": "languoid_list_cache",
        "data_size": "~2-3MB for 10,000 languoids (acceptable)",
        "browser_limit": "5-10MB typical sessionStorage limit",
        "cleanup": "Automatic on tab close"
      },
      
      "multi_user_considerations": {
        "change_detection": "Smart invalidation via last_modified timestamp",
        "scenario": "User A edits languoid, User B's cache automatically invalidates on next access",
        "delay": "< 10 minutes (TTL) + next access",
        "acceptable_for": "Staff-facing internal tool (not public-facing real-time system)"
      },
      
      "applicability_constraints": {
        "why_languoid_specific": [
          "Bounded dataset size (~10,000 records max = ~2-3MB)",
          "Expensive initial load (3 seconds due to hierarchical processing)",
          "Frequent navigation back-and-forth (detail → list → detail)",
          "Low update frequency relative to read frequency",
          "Staff tool (not public-facing, acceptable slight staleness)"
        ],
        "do_not_apply_to": [
          "Items list (100,000+ records = too large for client-side caching)",
          "Collections list (if dataset grows large)",
          "Real-time collaborative editing scenarios",
          "Public-facing data that must be immediately consistent"
        ],
        "when_to_consider": [
          "Bounded dataset (< 10,000 records)",
          "Expensive load time (> 2 seconds)",
          "Frequent navigation patterns",
          "Low update frequency",
          "Staff/internal tool (not public)"
        ],
        "decision_rule": "Always consult project manager before applying this pattern to other models"
      },
      
      "files_modified": [
        "app/internal_api/views.py (added last-modified endpoint)",
        "frontend/src/services/api.ts (added getLastModified method)",
        "frontend/src/contexts/LanguoidCacheContext.tsx (new file)",
        "frontend/src/App.tsx (wrapped with LanguoidCacheProvider)",
        "frontend/src/components/languoids/LanguoidsList.tsx (integrated cache)"
      ],
      
      "testing_scenarios": [
        "Navigate to list → detail → list (should be instant)",
        "Edit languoid in detail view → return to list (should auto-refresh)",
        "Wait 10 minutes → return to list (should auto-refresh)",
        "Click refresh button (should force reload)",
        "Refresh page → return to list (should use sessionStorage cache if fresh)",
        "Open in new tab → list loads fresh (separate session)",
        "Multiple users: User A edits, User B's cache invalidates on next access"
      ],
      
      "console_logging": {
        "enabled": true,
        "prefix": "[LanguoidCache]",
        "messages": [
          "Cache hit/miss decisions",
          "TTL age calculations",
          "Timestamp comparison results",
          "sessionStorage save/load events",
          "Helpful for debugging cache behavior"
        ]
      },
      
      "confidence_level": "high",
      "status": "completed",
      "production_ready": true
    },
    
    "server_side_cache_warming": {
      "change_id": "languoid_cache_002",
      "change_type": "performance_optimization",
      "date_implemented": "2025-10-28",
      "description": "Implemented warm cache strategy with proactive background refresh to eliminate user wait times on production (22-second initial load)",
      "motivation": "Production server takes 22 seconds to load full languoid list. Basic caching would cause frequent cache misses. Warm cache ensures users NEVER wait.",
      
      "problem_solved": {
        "production_bottleneck": "22-second load time for 10,000 languoids with hierarchical processing",
        "cache_miss_issue": "Staff browse sporadically (not continuously), so basic 10-min TTL cache would expire between sessions, causing frequent 22-second waits",
        "user_expectation": "Staff expect instant loading when returning to list page"
      },
      
      "solution": {
        "approach": "Warm Cache / Proactive Refresh",
        "key_insight": "Run the expensive 22-second query in the background BEFORE users request it",
        "user_experience": "Users never experience the 22-second wait - cache is always warm"
      },
      
      "implementation_components": {
        "1_redis_cache_backend": {
          "file": "app/archive/settings.py",
          "change": "Switched from LocMemCache to RedisCache",
          "before": "django.core.cache.backends.locmem.LocMemCache (per-worker, lost on restart)",
          "after": "django.core.cache.backends.redis.RedisCache (shared across workers, persistent)",
          "django_version_note": "Requires Django 4.0+. Django 5.0.14 deployed.",
          "benefits": [
            "Cache shared across all Gunicorn workers",
            "Survives Gunicorn/Django restarts",
            "Much faster than LocMem (microsecond access)",
            "Already running for Celery, no infrastructure changes needed"
          ],
          "configuration": {
            "backend": "django.core.cache.backends.redis.RedisCache",
            "location": "redis://:<password>@redis:6379/0",
            "timeout": 600,
            "max_connections": 50,
            "key_prefix": "archive"
          }
        },
        
        "2_cache_warming_task": {
          "file": "app/metadata/tasks.py",
          "task": "warm_languoid_list_cache",
          "description": "Background Celery task that rebuilds languoid list cache",
          "how_it_works": [
            "Acquires distributed lock (prevents concurrent rebuilds)",
            "Queries all 10,000 languoids with optimized select_related/prefetch_related",
            "Serializes to JSON (same format as API response)",
            "Stores in Redis with 10-minute TTL",
            "Releases lock",
            "Takes ~22 seconds but runs in background (users don't wait)"
          ],
          "triggered_by": [
            "Celery Beat schedule (every 9 minutes - proactive)",
            "Django signal on languoid save/delete (immediate invalidation)",
            "Django startup (optional - can warm on boot)"
          ],
          "lock_mechanism": "Redis-based lock prevents duplicate rebuilds if signal fires during scheduled rebuild"
        },
        
        "3_celery_beat_schedule": {
          "file": "app/archive/settings.py",
          "schedule": "Every 9 minutes (crontab(minute='*/9'))",
          "rationale": "Cache TTL is 10 minutes, so 9-minute refresh ensures cache never expires",
          "priority": 5,
          "task_flow": [
            "9:00 AM - Task runs, rebuilds cache (22 seconds)",
            "9:09 AM - Task runs, rebuilds cache (22 seconds) - old cache still valid",
            "9:18 AM - Task runs, rebuilds cache (22 seconds)",
            "... continues forever"
          ],
          "user_impact": "User can access list at ANY time and cache is fresh (< 1 minute old)"
        },
        
        "4_signal_invalidation": {
          "file": "app/metadata/signals.py",
          "signal": "invalidate_languoid_list_cache",
          "triggers": ["post_save", "post_delete on Languoid model"],
          "behavior": [
            "User saves/deletes a languoid",
            "Signal fires immediately",
            "Invalidates cache (deletes from Redis)",
            "Triggers background rebuild task (priority 8 - high)",
            "Task completes in ~22 seconds",
            "User navigates back to list - cache is fresh"
          ],
          "edge_case": "If user navigates back before rebuild completes (unlikely), they hit cache miss (22s wait)"
        },
        
        "5_viewset_cache_integration": {
          "file": "app/internal_api/views.py",
          "method": "InternalLanguoidViewSet.list()",
          "logic": [
            "Check if request is for full list (page_size=10000&hierarchical=true)",
            "If yes: Check Redis cache",
            "If cache hit: Return immediately (< 1 second)",
            "If cache miss: Build and cache (22 seconds) - rare!",
            "If no: Use normal pagination (not cached)"
          ],
          "cache_key": "languoid_list_full",
          "logging": "All cache hits/misses logged for monitoring"
        },
        
        "6_utility_function": {
          "file": "app/metadata/tasks.py",
          "function": "build_languoid_list_cache()",
          "description": "Shared utility for building cached response",
          "used_by": [
            "warm_languoid_list_cache task (background)",
            "InternalLanguoidViewSet.list() (on rare cache miss)"
          ],
          "optimization": "Uses select_related/prefetch_related for efficient query"
        },
        
        "7_startup_warming": {
          "file": "app/metadata/apps.py",
          "method": "MetadataConfig.ready()",
          "description": "Django startup signal that immediately warms cache",
          "how_it_works": [
            "Django app initialization completes",
            "ready() method fires automatically",
            "Triggers warm_languoid_list_cache task (priority 9)",
            "Task runs async in Celery (doesn't block Django)",
            "Cache is warm within ~22-30 seconds of startup"
          ],
          "benefits": [
            "First user always gets fast load",
            "No waiting for first Celery Beat schedule",
            "Predictable performance after deployments",
            "Graceful error handling (won't crash Django)"
          ],
          "error_handling": "Wrapped in try-except, logs warning but doesn't prevent Django startup"
        }
      },
      
      "cache_lifecycle": {
        "initial_state": "Cache empty (no data)",
        "on_startup": [
          "Django starts",
          "MetadataConfig.ready() fires",
          "Cache warming task triggered (priority 9)",
          "Task completes in ~22 seconds (background)",
          "Cache is warm - users can now load in < 1 second",
          "Celery Beat scheduler also starts",
          "Every 9 minutes: Scheduled refresh maintains warm cache"
        ],
        "during_normal_operation": [
          "Every 9 minutes: Background task refreshes cache",
          "Users always see data < 1 minute old",
          "Zero downtime (old cache used until new one ready)",
          "No user ever experiences 22-second wait"
        ],
        "on_edit": [
          "User edits languoid",
          "Signal invalidates cache immediately",
          "Background task rebuilds (22 seconds)",
          "If user returns before rebuild: Cache miss (22s wait)",
          "If user returns after rebuild: Cache hit (< 1s)"
        ],
        "on_redis_restart": [
          "Redis restarts (cache lost)",
          "Next user hits cache miss (22 seconds)",
          "Background task immediately warms cache",
          "Subsequent users: Cache hit (< 1 second)"
        ]
      },
      
      "performance_metrics": {
        "before_server_cache": {
          "first_load": "22 seconds (production)",
          "return_navigation_before_client_cache": "22 seconds (every time)",
          "return_navigation_after_client_cache": "< 100ms (client-side only)"
        },
        "after_server_cache": {
          "first_load_cold_cache": "22 seconds (rare - only after Redis restart)",
          "first_load_warm_cache": "< 1 second (typical)",
          "return_navigation": "< 100ms (client cache) or < 1s (server cache)",
          "after_edit_before_rebuild": "22 seconds (if user returns immediately)",
          "after_edit_after_rebuild": "< 1 second",
          "cache_hit_rate": "Expected 99%+ (cache almost always warm)"
        }
      },
      
      "synergy_with_client_cache": {
        "double_caching_strategy": [
          "Layer 1: Client-side cache (LanguoidCacheContext)",
          "Layer 2: Server-side cache (Redis)",
          "Layer 3: Database (expensive query)"
        ],
        "request_flow": {
          "user_first_visit": "Client miss → Server cache (< 1s) → Client caches",
          "user_return_navigation": "Client cache hit (< 100ms) - never hits server",
          "user_after_10_min": "Client invalidated → Server cache (< 1s) → Client caches",
          "user_edits_languoid": "Both caches invalidated → Server rebuilds in background",
          "user_b_different_browser": "Client miss → Server cache hit (< 1s) → Client caches"
        },
        "result": "Lightning fast for all users after initial load"
      },
      
      "why_warm_cache_vs_basic_cache": {
        "basic_cache_problem": {
          "scenario": "10-minute TTL, first user after expiration waits 22 seconds",
          "frequency": "High (staff browse sporadically, not continuously)",
          "user_experience": "Frustrating unpredictable slow loads"
        },
        "warm_cache_solution": {
          "scenario": "Background task runs every 9 minutes, cache never expires",
          "frequency": "Never (users never trigger rebuild)",
          "user_experience": "Always fast, predictable performance"
        },
        "analogy": "Like warming up a car engine before the driver gets in vs. making driver wait for engine to warm up"
      },
      
      "dependencies_added": [
        "django-redis==5.4.0 (Redis cache backend for Django)",
        "hiredis==3.0.0 (C parser for Redis, 10x faster than pure Python)"
      ],
      
      "files_modified": [
        "app/archive/settings.py (CACHES config, CELERY_BEAT_SCHEDULE)",
        "app/metadata/tasks.py (warm cache task, utility function, invalidation task)",
        "app/metadata/signals.py (cache invalidation signal)",
        "app/metadata/apps.py (startup cache warming)",
        "app/internal_api/views.py (list() method override for caching)",
        "app/requirements.txt (added django-redis, hiredis)"
      ],
      
      "monitoring_and_debugging": {
        "log_prefix": "[Cache Warming] or [Startup]",
        "key_log_messages": [
          "[Startup] Languoid list cache warming task triggered",
          "Cache rebuilt successfully in X seconds",
          "Cache hit! Returning N languoids from cache",
          "Cache miss! Building languoid list (this will be slow)",
          "Cache invalidated (after languoid edit)",
          "Another cache build is in progress, skipping (lock held)"
        ],
        "redis_monitoring": "Use redis-cli to inspect cache: KEYS archive:* | GET archive:languoid_list_full"
      },
      
      "deployment_notes": {
        "requires": [
          "Redis must be running (already configured in docker-compose.private.yml)",
          "Celery worker must be running (to execute warming task)",
          "Celery Beat must be running (for periodic refresh)",
          "Run: pipenv install (to install django-redis + hiredis)",
          "Django 5.0.14+ uses built-in RedisCache backend"
        ],
        "first_deploy": [
          "Run pipenv install to install django-redis",
          "Restart Django: docker-compose restart web",
          "Django starts and immediately triggers cache warming",
          "Task completes in ~22-30 seconds",
          "First user (after warm completes) gets < 1 second load",
          "Celery Beat schedule maintains warm cache every 9 minutes"
        ],
        "rollback_safe": "If issues arise, can revert to LocMemCache without data loss",
        "troubleshooting": {
          "error_connection_refused": "Error: 'Connection refused' → Check Redis is running: docker ps | grep redis",
          "error_no_django_redis": "Error: 'No module named django_redis' → Run: pipenv install django-redis",
          "cache_not_warming": "Check Celery logs: docker-compose logs celery | grep 'Cache Warming'"
        }
      },
      
      "extensibility_and_reusability": {
        "infrastructure_is_reusable": true,
        "description": "The warm cache implementation is designed as reusable infrastructure that can be applied to any expensive API endpoint",
        
        "shared_infrastructure_components": {
          "redis_cache_backend": {
            "status": "Already configured and running",
            "file": "app/archive/settings.py - CACHES configuration",
            "scope": "Shared by all models - no per-model setup needed",
            "capacity": "Redis can handle hundreds of cached endpoints simultaneously",
            "key_namespace": "Uses 'archive:' prefix to prevent key collisions"
          },
          "celery_infrastructure": {
            "status": "Already configured and running",
            "components": [
              "Celery worker (executes background tasks)",
              "Celery Beat (schedules periodic tasks)",
              "Redis as broker and result backend"
            ],
            "scope": "Shared task queue - just add new tasks",
            "priority_system": "1-10 priority levels available for task scheduling"
          }
        },
        
        "five_step_pattern_for_new_models": {
          "overview": "Copy-paste pattern to cache any expensive API endpoint",
          "estimated_time": "15-30 minutes per new cached endpoint",
          
          "step_1_build_function": {
            "location": "app/metadata/tasks.py",
            "purpose": "Encapsulates the expensive query logic",
            "template": "def build_<model>_<view>_cache():",
            "example": "def build_item_list_cache():",
            "implementation": [
              "Query model with select_related/prefetch_related",
              "Serialize to JSON using DRF serializer",
              "Return serialized data",
              "This function is used by both warming task AND ViewSet fallback"
            ],
            "code_pattern": {
              "imports": "from internal_api.serializers import ModelSerializer",
              "query": "queryset = Model.objects.select_related(...).all()",
              "serialize": "serializer = ModelSerializer(queryset, many=True)",
              "return": "return serializer.data"
            }
          },
          
          "step_2_warming_task": {
            "location": "app/metadata/tasks.py",
            "purpose": "Background Celery task that rebuilds cache",
            "template": "@shared_task(bind=True, max_retries=3)\ndef warm_<model>_<view>_cache(self):",
            "example": "@shared_task\ndef warm_item_list_cache():",
            "implementation": [
              "Acquire distributed lock (prevents concurrent rebuilds)",
              "Call build function from step 1",
              "Store in Redis cache with chosen TTL",
              "Release lock",
              "Return success/failure status"
            ],
            "key_decisions": {
              "cache_key": "Choose unique key like '<model>_<view>_full'",
              "ttl": "Usually 600 seconds (10 minutes)",
              "lock_key": "Add '_lock' suffix to cache_key",
              "lock_timeout": "Set higher than expected query time (e.g., 120 seconds)"
            },
            "code_pattern": {
              "lock": "lock_acquired = cache.add(lock_key, 'locked', timeout=120)",
              "build": "data = build_<model>_cache()",
              "cache": "cache.set(cache_key, data, timeout=600)",
              "unlock": "cache.delete(lock_key)"
            }
          },
          
          "step_3_celery_beat_schedule": {
            "location": "app/archive/settings.py - CELERY_BEAT_SCHEDULE",
            "purpose": "Proactive periodic refresh to keep cache warm",
            "template": "'warm-<model>-cache': { 'task': '...', 'schedule': crontab(...) }",
            "example": "'warm-item-list-cache': { 'task': 'metadata.tasks.warm_item_list_cache', 'schedule': crontab(minute='*/9') }",
            "schedule_guidelines": {
              "frequency": "Set 1 minute less than cache TTL (e.g., 9 min for 10 min TTL)",
              "rationale": "Ensures cache never expires between refreshes",
              "priority": "Use priority 5 (medium) for scheduled maintenance tasks"
            }
          },
          
          "step_4_signal_invalidation": {
            "location": "app/metadata/signals.py",
            "purpose": "Invalidate and rebuild cache immediately when data changes",
            "template": "@receiver(post_save, sender=Model)\n@receiver(post_delete, sender=Model)\ndef invalidate_<model>_cache(...):",
            "example": "@receiver(post_save, sender=Item)\ndef invalidate_item_cache():",
            "implementation": [
              "Listen for post_save and post_delete signals",
              "Delete cache key from Redis",
              "Trigger background rebuild task (high priority)",
              "Task completes in background (user doesn't wait)"
            ],
            "priority_guidelines": "Use priority 8 (high) - user just made edit, wants fresh data soon"
          },
          
          "step_5_viewset_integration": {
            "location": "app/internal_api/views.py - ModelViewSet",
            "purpose": "Check cache before expensive query",
            "template": "def list(self, request):",
            "example": "class InternalItemViewSet:\n    def list(self, request):",
            "implementation": [
              "Detect if request matches cached endpoint (e.g., page_size=10000)",
              "Check Redis cache for key",
              "If hit: Return cached data immediately",
              "If miss: Call build function, cache result, return"
            ],
            "conditional_caching": {
              "best_practice": "Only cache specific request patterns (e.g., full list)",
              "example": "is_full_list = request.query_params.get('page_size') == '10000'",
              "fallback": "For non-cached requests, use super().list() for normal pagination"
            }
          },
          
          "optional_step_6_startup_warming": {
            "location": "app/metadata/apps.py - AppConfig.ready()",
            "purpose": "Warm cache immediately on Django startup",
            "when_to_add": "If first user after restart must have fast load",
            "implementation": "warm_<model>_cache.apply_async(priority=9)",
            "note": "Only add if startup warming is critical for user experience"
          }
        },
        
        "when_to_consider_caching_api_endpoints": {
          "strong_candidates": {
            "criteria": "All of these conditions should be true",
            "conditions": [
              "Query takes > 5 seconds consistently",
              "Data is read much more often than written (high read:write ratio)",
              "Results are sharable across users (not per-user customized)",
              "Dataset is bounded (won't grow to exhaust Redis memory)",
              "Staleness tolerance: Users can tolerate data up to 10 minutes old"
            ],
            "examples": [
              "Full item list (if it becomes slow as dataset grows)",
              "Collection list with item counts (expensive aggregations)",
              "Statistics/dashboard data (computed aggregates)",
              "Hierarchical data structures (like languoid tree)"
            ]
          },
          
          "evaluate_these_factors": {
            "query_cost": {
              "measure": "Profile with django-debug-toolbar or django-silk",
              "threshold": "If > 5 seconds, strong candidate",
              "calculation": "Include DB time + serialization time"
            },
            "update_frequency": {
              "measure": "How often is this data modified?",
              "low_frequency": "< 10 edits per hour → Good candidate",
              "high_frequency": "> 100 edits per hour → Poor candidate (cache constantly invalidates)"
            },
            "access_patterns": {
              "measure": "How often is this endpoint called?",
              "high_traffic": "Multiple users or frequent page loads → Good candidate",
              "low_traffic": "Rarely accessed → Poor candidate (cache overhead not worth it)"
            },
            "dataset_size": {
              "measure": "How much memory will cached response consume?",
              "acceptable": "< 10 MB per cache key",
              "warning": "10-50 MB - monitor Redis memory usage",
              "problematic": "> 50 MB - consider pagination or smaller cache scope"
            },
            "staleness_tolerance": {
              "question": "Can users tolerate seeing data that's up to 10 minutes old?",
              "yes": "Good candidate (most list views, dashboards)",
              "no": "Poor candidate (real-time data, financial transactions)"
            }
          },
          
          "poor_candidates_do_not_cache": {
            "criteria": "Any of these conditions makes caching inappropriate",
            "scenarios": [
              "Per-user data (e.g., 'my items' list filtered by user)",
              "Search results with user-specific query terms",
              "Real-time data that must always be current",
              "Paginated endpoints where cache would need hundreds of keys",
              "Data that changes more often than it's read",
              "Queries that are already fast (< 1 second)",
              "Unbounded datasets (e.g., logs, audit trails that grow indefinitely)"
            ]
          }
        },
        
        "example_future_cache_candidates": {
          "item_list_full": {
            "endpoint": "GET /internal/v1/items/?page_size=10000",
            "current_status": "May become slow as dataset grows to 50,000+ items",
            "when_to_cache": "If query time exceeds 5 seconds",
            "implementation": "Follow 5-step pattern, copy from languoid implementation",
            "considerations": [
              "Items updated more frequently than languoids",
              "May need shorter TTL (e.g., 5 minutes instead of 10)",
              "Monitor cache invalidation frequency"
            ]
          },
          
          "collection_list_with_counts": {
            "endpoint": "GET /internal/v1/collections/ (with item counts)",
            "why_expensive": "Aggregating item counts across collections is slow",
            "when_to_cache": "If aggregation takes > 3 seconds",
            "implementation": "Cache the collection list with pre-computed counts",
            "invalidation_strategy": "Invalidate when items OR collections are saved/deleted"
          },
          
          "statistics_dashboard": {
            "endpoint": "Hypothetical GET /internal/v1/statistics/",
            "why_expensive": "Aggregates across multiple models",
            "when_to_cache": "Always - dashboards are perfect cache candidates",
            "implementation": "Long TTL (30 minutes), refresh every 29 minutes",
            "invalidation_strategy": "Optional - can rely on TTL for statistics"
          }
        },
        
        "measuring_before_and_after": {
          "before_caching": {
            "step_1": "Profile endpoint with django-debug-toolbar",
            "step_2": "Note query time (e.g., 22 seconds)",
            "step_3": "Check query frequency in logs",
            "step_4": "Calculate: queries_per_hour * seconds_per_query = total_server_time"
          },
          "after_caching": {
            "step_1": "Deploy cache implementation",
            "step_2": "Monitor cache hit rate in logs (should be > 90%)",
            "step_3": "Measure response time (should be < 1 second)",
            "step_4": "Calculate savings: total_server_time * cache_hit_rate"
          },
          "success_metrics": {
            "cache_hit_rate": "> 90% (cache is being used effectively)",
            "response_time": "< 1 second for cached responses",
            "background_rebuild_time": "< 60 seconds (reasonable for background)",
            "redis_memory_usage": "< 100 MB total (sustainable)"
          }
        },
        
        "best_practices_and_gotchas": {
          "best_practices": [
            "Always use distributed locks to prevent concurrent rebuilds",
            "Log cache hits/misses for monitoring",
            "Use descriptive cache keys (e.g., 'languoid_list_full' not 'cache1')",
            "Set TTL slightly longer than refresh interval (10 min TTL, 9 min refresh)",
            "Use priority 8-9 for user-triggered invalidation, 5 for scheduled refresh",
            "Test cache invalidation by editing data and checking logs",
            "Monitor Redis memory usage with redis-cli INFO memory"
          ],
          "gotchas_to_avoid": [
            "Don't cache per-user data (cache won't be shared)",
            "Don't cache if data changes constantly (cache always stale)",
            "Don't cache unbounded datasets (Redis memory exhaustion)",
            "Don't forget to invalidate cache on model saves/deletes",
            "Don't use same cache key for different endpoints",
            "Don't set TTL too short (defeats purpose) or too long (stale data)",
            "Don't block user requests - always use async tasks for rebuilding"
          ],
          "common_mistakes": {
            "forgetting_lock": "Results in duplicate rebuilds, wasted resources",
            "wrong_priority": "Low priority = slow invalidation after edits",
            "no_fallback": "If cache miss, ViewSet must still handle query",
            "cache_key_collision": "Different endpoints using same key",
            "missing_signal": "Cache never invalidates, data becomes stale"
          }
        },
        
        "redis_capacity_planning": {
          "current_usage": {
            "languoid_list": "~2-3 MB (10,000 records)",
            "redis_memory_limit": "Typically 512 MB - 2 GB on TrueNAS Scale"
          },
          "capacity_calculation": {
            "formula": "cached_endpoints * avg_response_size * 1.5 (overhead)",
            "example": "5 endpoints * 3 MB * 1.5 = 22.5 MB (plenty of headroom)"
          },
          "monitoring": {
            "command": "docker exec redis_private redis-cli INFO memory",
            "watch_for": "used_memory_human, maxmemory_human",
            "warning_threshold": "If used_memory > 80% of maxmemory"
          }
        },
        
        "decision_framework": {
          "step_1_measure": "Profile the endpoint - how slow is it?",
          "step_2_evaluate": "Check criteria: read frequency, update frequency, shareable, bounded",
          "step_3_estimate": "Calculate potential savings vs. implementation cost",
          "step_4_consult": "Discuss with project manager - is it worth it?",
          "step_5_implement": "Follow 5-step pattern, takes 15-30 minutes",
          "step_6_monitor": "Watch logs, measure hit rate, verify performance gain",
          "decision_rule": "If query > 5 seconds AND meets criteria → cache it"
        }
      },
      
      "applicability_to_other_models": {
        "when_to_consider": [
          "Model has expensive query (> 5 seconds)",
          "Query results are stable (low update frequency)",
          "Multiple users or repeated access patterns",
          "Dataset is bounded (can fit in Redis memory)"
        ],
        "when_not_to_use": [
          "Frequently updated data (cache would constantly invalidate)",
          "Per-user customized data (can't share cache)",
          "Unbounded dataset (would exhaust Redis memory)",
          "Query is already fast (< 1 second - caching adds complexity)"
        ],
        "decision_rule": "Always consult project manager and measure before/after performance"
      },
      
      "confidence_level": "high",
      "status": "completed",
      "production_ready": true
    },
    
    "related_context": [
      "coding_patterns.json (EditableField patterns)",
      "stage_0_react_migration.json (list view patterns)",
      "glottocode-url-implementation.json (URL routing with glottocodes)",
      "login-ux-improvements.json (navigation patterns)"
    ],
    
    "confidence_level": "high",
    "status": "completed"
  }
}

